{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-17T09:15:59.104533Z","iopub.execute_input":"2023-06-17T09:15:59.105497Z","iopub.status.idle":"2023-06-17T09:15:59.137261Z","shell.execute_reply.started":"2023-06-17T09:15:59.105434Z","shell.execute_reply":"2023-06-17T09:15:59.135938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from transformers import BertTokenizer, BertForSequenceClassification\n# import numpy as np\n\n# # Load pre-trained GloVe embeddings\n# glove_path = '/kaggle/input/glove-text-file/glove.6B.100d.txt'  # Path to your GloVe embeddings file\n# embedding_matrix[index] = np.reshape(glove_embeddings[word], (1, -1))\n\n# glove_embeddings = {}\n\n# with open(glove_path, 'r', encoding='utf-8') as f:\n#     for line in f:\n#         values = line.strip().split(' ')\n#         word = values[0]\n#         embeddings = np.asarray(values[1:], dtype=np.float32)\n#         glove_embeddings[word] = embeddings\n\n# # Load pre-trained BERT model and tokenizer\n# model_name = 'bert-base-uncased'\n# tokenizer = BertTokenizer.from_pretrained(model_name)\n# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# # Initialize BERT model's embedding layer with GloVe embeddings\n# vocab_size = tokenizer.vocab_size\n# embedding_dim = model.config.hidden_size\n\n# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n# for word, index in tokenizer.get_vocab().items():\n#     if word in glove_embeddings:\n#         embedding_matrix[index] = glove_embeddings[word]\n\n# model.bert.embeddings.word_embeddings.weight.data = torch.tensor(embedding_matrix)\n\n# # Example user query\n# user_query = \"machine learning\"\n\n# # Tokenize user query\n# query_tokens = tokenizer.tokenize(user_query)\n# query_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n# query_ids = tokenizer.build_inputs_with_special_tokens(query_ids)\n\n# # Convert query to tensor\n# query_tensor = torch.tensor([query_ids])\n\n# # Forward pass through the BERT model\n# outputs = model(query_tensor)\n\n# # Get the predicted probabilities and predicted label\n# probs = torch.softmax(outputs[0], dim=1)\n# predicted_label = torch.argmax(probs, dim=1).item()\n\n# # Convert label index to label text\n# label_map = {0: \"Not Relevant\", 1: \"Relevant\"}  # Modify as per your task\n# predicted_label_text = label_map[predicted_label]\n\n# # Print results\n# print(\"User Query:\", user_query)\n# print(\"Predicted Label:\", predicted_label_text)\n# print(\"Predicted Probabilities:\", probs.tolist())\n","metadata":{"execution":{"iopub.status.busy":"2023-06-16T01:23:51.528616Z","iopub.execute_input":"2023-06-16T01:23:51.529074Z","iopub.status.idle":"2023-06-16T01:23:51.543515Z","shell.execute_reply.started":"2023-06-16T01:23:51.529035Z","shell.execute_reply":"2023-06-16T01:23:51.542344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade keras \n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:16:14.813321Z","iopub.execute_input":"2023-06-17T09:16:14.813735Z","iopub.status.idle":"2023-06-17T09:16:27.587751Z","shell.execute_reply.started":"2023-06-17T09:16:14.813702Z","shell.execute_reply":"2023-06-17T09:16:27.586324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:16:35.349147Z","iopub.execute_input":"2023-06-17T09:16:35.349557Z","iopub.status.idle":"2023-06-17T09:16:46.691744Z","shell.execute_reply.started":"2023-06-17T09:16:35.349511Z","shell.execute_reply":"2023-06-17T09:16:46.690560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nprint(keras.__version__) \n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:16:53.576219Z","iopub.execute_input":"2023-06-17T09:16:53.576628Z","iopub.status.idle":"2023-06-17T09:17:01.941670Z","shell.execute_reply.started":"2023-06-17T09:16:53.576591Z","shell.execute_reply":"2023-06-17T09:17:01.940467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom tqdm import tqdm\nimport os\nimport spacy\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nfrom collections import defaultdict\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import (LSTM, \n                                      Embedding, \n                                      BatchNormalization,\n                                      Dense, \n                                      TimeDistributed, \n                                      Dropout, \n                                      Bidirectional,\n                                      Flatten, \n                                      GlobalMaxPool1D)\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.metrics import (precision_score, \n                             recall_score, \n                             f1_score, \n                             classification_report,\n                             accuracy_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:17:07.347870Z","iopub.execute_input":"2023-06-17T09:17:07.348588Z","iopub.status.idle":"2023-06-17T09:17:15.661336Z","shell.execute_reply.started":"2023-06-17T09:17:07.348557Z","shell.execute_reply":"2023-06-17T09:17:15.660280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/querycodeexp/test_data.csv\",nrows=500 )\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:17:20.961343Z","iopub.execute_input":"2023-06-17T09:17:20.962183Z","iopub.status.idle":"2023-06-17T09:17:21.036252Z","shell.execute_reply.started":"2023-06-17T09:17:20.962148Z","shell.execute_reply":"2023-06-17T09:17:21.035296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/querycodeexp/train_data.csv\", nrows=1000)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:17:24.162350Z","iopub.execute_input":"2023-06-17T09:17:24.163306Z","iopub.status.idle":"2023-06-17T09:17:24.283323Z","shell.execute_reply.started":"2023-06-17T09:17:24.163267Z","shell.execute_reply":"2023-06-17T09:17:24.282364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balance_counts = train_df.groupby('docstring')['docstring'].agg('count').values\nbalance_counts","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:17:29.835974Z","iopub.execute_input":"2023-06-17T09:17:29.836676Z","iopub.status.idle":"2023-06-17T09:17:29.864809Z","shell.execute_reply.started":"2023-06-17T09:17:29.836631Z","shell.execute_reply":"2023-06-17T09:17:29.863904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balance_counts = test_df.groupby('url')['url'].agg('count').values\nbalance_counts","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:17:35.743447Z","iopub.execute_input":"2023-06-17T09:17:35.744666Z","iopub.status.idle":"2023-06-17T09:17:35.757565Z","shell.execute_reply.started":"2023-06-17T09:17:35.744628Z","shell.execute_reply":"2023-06-17T09:17:35.756624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=['docstring'],\n    y=[balance_counts[0]],\n    name='docstring',\n    text=[balance_counts[0]],\n    textposition='auto',\n))\nfig.add_trace(go.Bar(\n    x=['url'],\n    y=[balance_counts[1]],\n    name='url',\n    text=[balance_counts[1]],\n    textposition='auto',\n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Dataset distribution by target</span>'\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:17:40.738635Z","iopub.execute_input":"2023-06-17T09:17:40.739078Z","iopub.status.idle":"2023-06-17T09:17:40.906684Z","shell.execute_reply.started":"2023-06-17T09:17:40.739039Z","shell.execute_reply":"2023-06-17T09:17:40.905562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Special thanks to https://www.kaggle.com/tanulsingh077 for this function\ndef clean_docstring(docstring):\n    '''Make docstring lowercase, remove docstring in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    docstring = str(docstring).lower()\n    docstring = re.sub('\\[.*?\\]', '', docstring)\n    docstring = re.sub('https?://\\S+|www\\.\\S+', '', docstring)\n    docstring = re.sub('<.*?>+', '', docstring)\n    docstring = re.sub('[%s]' % re.escape(string.punctuation), '', docstring)\n    docstring = re.sub('\\n', '', docstring)\n    docstring = re.sub('\\w*\\d\\w*', '', docstring)\n    return docstring  ","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:17:49.072457Z","iopub.execute_input":"2023-06-17T09:17:49.073208Z","iopub.status.idle":"2023-06-17T09:17:49.082881Z","shell.execute_reply.started":"2023-06-17T09:17:49.073162Z","shell.execute_reply":"2023-06-17T09:17:49.081373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['docstring_clean'] = train_df['docstring'].apply(clean_docstring)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:17:58.996473Z","iopub.execute_input":"2023-06-17T09:17:58.997414Z","iopub.status.idle":"2023-06-17T09:17:59.159937Z","shell.execute_reply.started":"2023-06-17T09:17:58.997382Z","shell.execute_reply":"2023-06-17T09:17:59.158860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['docstring_clean'] = test_df['docstring'].apply(clean_docstring)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:18:10.216574Z","iopub.execute_input":"2023-06-17T09:18:10.216934Z","iopub.status.idle":"2023-06-17T09:18:10.285529Z","shell.execute_reply.started":"2023-06-17T09:18:10.216908Z","shell.execute_reply":"2023-06-17T09:18:10.284422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nmore_stopwords = ['u', 'im', 'c']\nstop_words = stop_words + more_stopwords\n\ndef remove_stopwords(docstring):\n    docstring = ' '.join(word for word in docstring.split(' ') if word not in stop_words)\n    return docstring\n    \ntest_df['docstring_clean'] = test_df['docstring_clean'].apply(remove_stopwords)\ntrain_df['docstring_clean'] = train_df['docstring_clean'].apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:18:28.558622Z","iopub.execute_input":"2023-06-17T09:18:28.558976Z","iopub.status.idle":"2023-06-17T09:18:28.892544Z","shell.execute_reply.started":"2023-06-17T09:18:28.558948Z","shell.execute_reply":"2023-06-17T09:18:28.891524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmer = nltk.SnowballStemmer(\"english\")\n\ndef stemm_docstring(docstring):\n    docstring = ' '.join(stemmer.stem(word) for word in docstring.split(' '))\n    return docstring","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:18:39.226232Z","iopub.execute_input":"2023-06-17T09:18:39.226818Z","iopub.status.idle":"2023-06-17T09:18:39.232877Z","shell.execute_reply.started":"2023-06-17T09:18:39.226779Z","shell.execute_reply":"2023-06-17T09:18:39.231725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['docstring_clean'] = test_df['docstring_clean'].apply(stemm_docstring)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:18:50.129885Z","iopub.execute_input":"2023-06-17T09:18:50.130765Z","iopub.status.idle":"2023-06-17T09:18:50.437435Z","shell.execute_reply.started":"2023-06-17T09:18:50.130722Z","shell.execute_reply":"2023-06-17T09:18:50.436496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['docstring_clean'] = train_df['docstring_clean'].apply(stemm_docstring)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:19:08.304843Z","iopub.execute_input":"2023-06-17T09:19:08.305795Z","iopub.status.idle":"2023-06-17T09:19:08.980103Z","shell.execute_reply.started":"2023-06-17T09:19:08.305761Z","shell.execute_reply":"2023-06-17T09:19:08.978796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(docstring):\n    # Clean puntuation, urls, and so on\n    docstring = clean_docstring(docstring)\n    # Remove stopwords\n    docstring = ' '.join(word for word in docstring.split(' ') if word not in stop_words)\n    # Stemm all the words in the sentence\n    docstring = ' '.join(stemmer.stem(word) for word in docstring.split(' '))\n    \n    return docstring","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:19:20.161028Z","iopub.execute_input":"2023-06-17T09:19:20.161418Z","iopub.status.idle":"2023-06-17T09:19:20.171262Z","shell.execute_reply.started":"2023-06-17T09:19:20.161390Z","shell.execute_reply":"2023-06-17T09:19:20.169989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['docstring_clean'] = test_df['docstring_clean'].apply(preprocess_data)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:19:39.035211Z","iopub.execute_input":"2023-06-17T09:19:39.035617Z","iopub.status.idle":"2023-06-17T09:19:39.404563Z","shell.execute_reply.started":"2023-06-17T09:19:39.035586Z","shell.execute_reply":"2023-06-17T09:19:39.403537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['docstring_clean'] = train_df['docstring_clean'].apply(preprocess_data)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:19:46.410705Z","iopub.execute_input":"2023-06-17T09:19:46.411060Z","iopub.status.idle":"2023-06-17T09:19:47.398094Z","shell.execute_reply.started":"2023-06-17T09:19:46.411032Z","shell.execute_reply":"2023-06-17T09:19:47.397109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle.fit(train_df['url'])\n\n\ntrain_df['url_encoded'] = le.transform(train_df['url'])\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:19:53.063781Z","iopub.execute_input":"2023-06-17T09:19:53.064176Z","iopub.status.idle":"2023-06-17T09:19:53.089279Z","shell.execute_reply.started":"2023-06-17T09:19:53.064147Z","shell.execute_reply":"2023-06-17T09:19:53.088253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\nle.fit(test_df['url'])\n\n\ntest_df['url_encoded'] = le.transform(test_df['url'])\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:19:59.372470Z","iopub.execute_input":"2023-06-17T09:19:59.372917Z","iopub.status.idle":"2023-06-17T09:19:59.393705Z","shell.execute_reply.started":"2023-06-17T09:19:59.372890Z","shell.execute_reply":"2023-06-17T09:19:59.392556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_x = test_df['docstring_clean']\ntest_y = test_df['url_encoded']\n\nprint(len(test_x), len(test_y))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:05.271499Z","iopub.execute_input":"2023-06-17T09:20:05.271981Z","iopub.status.idle":"2023-06-17T09:20:05.278770Z","shell.execute_reply.started":"2023-06-17T09:20:05.271944Z","shell.execute_reply":"2023-06-17T09:20:05.277677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = train_df['docstring_clean']\ntrain_y = train_df['url_encoded']","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:08.132072Z","iopub.execute_input":"2023-06-17T09:20:08.132464Z","iopub.status.idle":"2023-06-17T09:20:08.137454Z","shell.execute_reply.started":"2023-06-17T09:20:08.132435Z","shell.execute_reply":"2023-06-17T09:20:08.136297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# instantiate the vectorizer\nvect = CountVectorizer()\nvect.fit(train_x)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:13.892224Z","iopub.execute_input":"2023-06-17T09:20:13.892632Z","iopub.status.idle":"2023-06-17T09:20:13.963406Z","shell.execute_reply.started":"2023-06-17T09:20:13.892601Z","shell.execute_reply":"2023-06-17T09:20:13.962449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# instantiate the vectorizer\nvect = CountVectorizer()\nvect.fit(test_x)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:16.307106Z","iopub.execute_input":"2023-06-17T09:20:16.307460Z","iopub.status.idle":"2023-06-17T09:20:16.337605Z","shell.execute_reply.started":"2023-06-17T09:20:16.307431Z","shell.execute_reply":"2023-06-17T09:20:16.336583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test_dtm = vect.transform(test_x)\nx_train_dtm = vect.transform(train_x)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:21.487829Z","iopub.execute_input":"2023-06-17T09:20:21.488312Z","iopub.status.idle":"2023-06-17T09:20:21.548237Z","shell.execute_reply.started":"2023-06-17T09:20:21.488282Z","shell.execute_reply":"2023-06-17T09:20:21.547316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vect_tunned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:24.030185Z","iopub.execute_input":"2023-06-17T09:20:24.030572Z","iopub.status.idle":"2023-06-17T09:20:24.036536Z","shell.execute_reply.started":"2023-06-17T09:20:24.030540Z","shell.execute_reply":"2023-06-17T09:20:24.035425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\n\ntfidf_transformer.fit(x_train_dtm)\nx_train_tfidf = tfidf_transformer.transform(x_train_dtm)\n\nx_train_tfidf","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:26.550309Z","iopub.execute_input":"2023-06-17T09:20:26.550684Z","iopub.status.idle":"2023-06-17T09:20:26.570415Z","shell.execute_reply.started":"2023-06-17T09:20:26.550654Z","shell.execute_reply":"2023-06-17T09:20:26.569171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = train_df['docstring_clean']\ntarget = train_df['url_encoded']","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:30.636662Z","iopub.execute_input":"2023-06-17T09:20:30.637077Z","iopub.status.idle":"2023-06-17T09:20:30.642223Z","shell.execute_reply.started":"2023-06-17T09:20:30.637047Z","shell.execute_reply":"2023-06-17T09:20:30.640649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the length of our vocabulary\nword_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(texts)\n\nvocab_length = len(word_tokenizer.word_index) + 1\nvocab_length","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:32.501086Z","iopub.execute_input":"2023-06-17T09:20:32.501448Z","iopub.status.idle":"2023-06-17T09:20:32.555427Z","shell.execute_reply.started":"2023-06-17T09:20:32.501420Z","shell.execute_reply":"2023-06-17T09:20:32.554388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)\n\nlongest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\n\ntrain_padded_sentences = pad_sequences(\n    embed(texts), \n    length_long_sentence, \n    padding='post'\n)\n\ntrain_padded_sentences","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:34.807531Z","iopub.execute_input":"2023-06-17T09:20:34.807905Z","iopub.status.idle":"2023-06-17T09:20:35.342430Z","shell.execute_reply.started":"2023-06-17T09:20:34.807878Z","shell.execute_reply":"2023-06-17T09:20:35.341413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_dictionary = dict()\nembedding_dim = 100\n\n# Load GloVe 100D embeddings\nwith open('/kaggle/input/codetextexp/glove.6B.100d.txt') as fp:\n    for line in fp.readlines():\n        records = line.split()\n        word = records[0]\n        vector_dimensions = np.asarray(records[1:], dtype='float32')\n        embeddings_dictionary [word] = vector_dimensions\n\n# embeddings_dictionary","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:20:38.010022Z","iopub.execute_input":"2023-06-17T09:20:38.010387Z","iopub.status.idle":"2023-06-17T09:20:54.501895Z","shell.execute_reply.started":"2023-06-17T09:20:38.010357Z","shell.execute_reply":"2023-06-17T09:20:54.500830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #Now we will load embedding vectors of those words that appear in the\n# Glove dictionary. Others will be initialized to 0.\n\nembedding_matrix = np.zeros((vocab_length, embedding_dim))\n\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n        \nembedding_matrix","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:21:02.541966Z","iopub.execute_input":"2023-06-17T09:21:02.542340Z","iopub.status.idle":"2023-06-17T09:21:02.560745Z","shell.execute_reply.started":"2023-06-17T09:21:02.542310Z","shell.execute_reply":"2023-06-17T09:21:02.559631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n\n# Train the model\nnb.fit(x_train_dtm, train_y)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:21:10.398230Z","iopub.execute_input":"2023-06-17T09:21:10.398616Z","iopub.status.idle":"2023-06-17T09:21:10.490116Z","shell.execute_reply.started":"2023-06-17T09:21:10.398585Z","shell.execute_reply":"2023-06-17T09:21:10.488902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make class anf probability predictions\ny_pred_class = nb.predict(x_test_dtm)\ny_pred_prob = nb.predict_proba(x_test_dtm)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:21:16.538352Z","iopub.execute_input":"2023-06-17T09:21:16.538976Z","iopub.status.idle":"2023-06-17T09:21:16.575554Z","shell.execute_reply.started":"2023-06-17T09:21:16.538944Z","shell.execute_reply":"2023-06-17T09:21:16.574565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # calculate accuracy of class predictions\n# from sklearn import metrics\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# from sklearn.metrics import confusion_matrix\n# import numpy as np\n\n# confusion = confusion_matrix(test_y, y_pred_class)\n# print(confusion)\n\n# print(metrics.accuracy_score(test_y, y_pred_class))\n\n# conf_matrix(metrics.confusion_matrix(test_y, y_pred_class))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T11:08:10.811887Z","iopub.execute_input":"2023-06-09T11:08:10.812525Z","iopub.status.idle":"2023-06-09T11:08:10.817018Z","shell.execute_reply.started":"2023-06-09T11:08:10.812491Z","shell.execute_reply":"2023-06-09T11:08:10.816026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Calculate AUC\n# metrics.roc_auc_score(test_y, y_pred_prob, multi_class='ovr')","metadata":{"execution":{"iopub.status.busy":"2023-06-09T11:08:12.080073Z","iopub.execute_input":"2023-06-09T11:08:12.080419Z","iopub.status.idle":"2023-06-09T11:08:12.084510Z","shell.execute_reply.started":"2023-06-09T11:08:12.080393Z","shell.execute_reply":"2023-06-09T11:08:12.083589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('bow', CountVectorizer()), \n                 ('tfid', TfidfTransformer()),  \n                 ('model', MultinomialNB())])","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:21:24.263165Z","iopub.execute_input":"2023-06-17T09:21:24.263700Z","iopub.status.idle":"2023-06-17T09:21:24.278712Z","shell.execute_reply.started":"2023-06-17T09:21:24.263660Z","shell.execute_reply":"2023-06-17T09:21:24.277821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the pipeline with the data\npipe.fit(train_x, train_y)\n\ny_pred_class = pipe.predict(test_x)\n\n# print(metrics.accuracy_score(test_y, y_pred_class))\n\n# conf_matrix(metrics.confusion_matrix(test_y, y_pred_class))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:21:30.030548Z","iopub.execute_input":"2023-06-17T09:21:30.030950Z","iopub.status.idle":"2023-06-17T09:21:30.362304Z","shell.execute_reply.started":"2023-06-17T09:21:30.030920Z","shell.execute_reply":"2023-06-17T09:21:30.361255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\npipe = Pipeline([\n    ('bow', CountVectorizer()), \n    ('tfid', TfidfTransformer()),  \n    ('model', xgb.XGBClassifier(\n        learning_rate=0.1,\n        max_depth=7,\n        n_estimators=80,\n        use_label_encoder=False,\n        eval_metric='auc',\n        # colsample_bytree=0.8,\n        # subsample=0.7,\n        # min_child_weight=5,\n    ))\n])","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:21:33.622987Z","iopub.execute_input":"2023-06-17T09:21:33.623335Z","iopub.status.idle":"2023-06-17T09:21:33.804059Z","shell.execute_reply.started":"2023-06-17T09:21:33.623307Z","shell.execute_reply":"2023-06-17T09:21:33.803126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:21:46.559222Z","iopub.execute_input":"2023-06-17T09:21:46.559822Z","iopub.status.idle":"2023-06-17T09:21:46.564289Z","shell.execute_reply.started":"2023-06-17T09:21:46.559781Z","shell.execute_reply":"2023-06-17T09:21:46.563320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the pipeline with the data\npipe.fit(train_x, train_y)\n\ny_pred_class = pipe.predict(test_x)\ny_pred_train = pipe.predict(train_x)\n\nprint('Train: {}'.format(metrics.accuracy_score(train_y, y_pred_train)))\nprint('Test: {}'.format(metrics.accuracy_score(test_y, y_pred_class)))\n\n# conf_matrix(metrics.confusion_matrix(test_y, y_pred_class))","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:21:53.826293Z","iopub.execute_input":"2023-06-17T09:21:53.827140Z","iopub.status.idle":"2023-06-17T09:22:57.813548Z","shell.execute_reply.started":"2023-06-17T09:21:53.827104Z","shell.execute_reply":"2023-06-17T09:22:57.812269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BERT**","metadata":{}},{"cell_type":"code","source":"# import torch\n# from transformers import BertTokenizer, BertModel\n# import numpy as np\n\n# # Load pre-trained GloVe embeddings\n# glove_path = '/kaggle/input/glove-text-file/glove.6B.100d.txt'  # Path to your GloVe embeddings file\n# glove_embeddings = {}\n\n# with open(glove_path, 'r', encoding='utf-8') as f:\n#     for line in f:\n#         values = line.strip().split(' ')\n#         word = values[0]\n# #         embeddings = np.asarray(values[1:], dtype=np.float32)\n#         glove_embeddings[word] = embeddings\n\n# # Load pre-trained BERT model and tokenizer\n# model_name = 'bert-base-uncased'\n# tokenizer = BertTokenizer.from_pretrained(model_name)\n# model = BertModel.from_pretrained(model_name)\n\n# # Initialize projection layer\n# projection = torch.nn.Linear(100, 768)\n\n# # Example input\n# input_text = \"This is an example sentence.\"\n\n# # Tokenize input text\n# input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n# input_ids = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n\n# # Convert GloVe embeddings to tensor\n# embeddings_tensor = torch.tensor([glove_embeddings[word] for word in tokenizer.get_vocab() if word in glove_embeddings])\n\n# # Project GloVe embeddings to match BERT hidden size\n# projected_embeddings = projection(embeddings_tensor)\n\n# # Initialize BERT model's embedding layer with projected GloVe embeddings\n# model.embeddings.word_embeddings.weight.data = projected_embeddings\n\n# # Forward pass through the model\n# outputs = model(input_ids)\n\n# # Get the contextualized word embeddings for each token\n# contextual_embeddings = outputs.last_hidden_state\n\n# # Print the contextualized word embeddings\n# print(\"Input Text:\", input_text)\n# print(\"Contextualized Word Embeddings:\", contextual_embeddings)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:23:07.286362Z","iopub.execute_input":"2023-06-17T09:23:07.287045Z","iopub.status.idle":"2023-06-17T09:23:19.062332Z","shell.execute_reply.started":"2023-06-17T09:23:07.287007Z","shell.execute_reply":"2023-06-17T09:23:19.061219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:23:22.695250Z","iopub.execute_input":"2023-06-17T09:23:22.695683Z","iopub.status.idle":"2023-06-17T09:23:43.356255Z","shell.execute_reply.started":"2023-06-17T09:23:22.695648Z","shell.execute_reply":"2023-06-17T09:23:43.354945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:23:47.721407Z","iopub.execute_input":"2023-06-17T09:23:47.721806Z","iopub.status.idle":"2023-06-17T09:23:47.988731Z","shell.execute_reply.started":"2023-06-17T09:23:47.721772Z","shell.execute_reply":"2023-06-17T09:23:47.987773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nexcept:\n    strategy = tf.distribute.get_strategy()\n    \nprint('Number of replicas in sync: ', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:23:51.004813Z","iopub.execute_input":"2023-06-17T09:23:51.005338Z","iopub.status.idle":"2023-06-17T09:23:51.034161Z","shell.execute_reply.started":"2023-06-17T09:23:51.005291Z","shell.execute_reply":"2023-06-17T09:23:51.032796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef bert_encode(data, maximum_length) :\n    input_ids = []\n    attention_masks = []\n\n    for text in data:\n        encoded = tokenizer.encode_plus(\n            text, \n            add_special_tokens=True,\n            max_length=maximum_length,\n            pad_to_max_length=True,\n\n            return_attention_mask=True,\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids),np.array(attention_masks)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:23:57.343487Z","iopub.execute_input":"2023-06-17T09:23:57.344006Z","iopub.status.idle":"2023-06-17T09:23:58.108323Z","shell.execute_reply.started":"2023-06-17T09:23:57.343968Z","shell.execute_reply":"2023-06-17T09:23:58.107364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = test_df['docstring_clean']\ntarget = test_df['url_encoded']\n\ntrain_input_ids, train_attention_masks = bert_encode(texts,60)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:24:03.361682Z","iopub.execute_input":"2023-06-17T09:24:03.362049Z","iopub.status.idle":"2023-06-17T09:24:04.129674Z","shell.execute_reply.started":"2023-06-17T09:24:03.362018Z","shell.execute_reply":"2023-06-17T09:24:04.128724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_model(bert_model):\n    \n    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n\n    output = bert_model([input_ids,attention_masks])\n    output = output[1]\n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:24:08.536079Z","iopub.execute_input":"2023-06-17T09:24:08.536529Z","iopub.status.idle":"2023-06-17T09:24:08.546856Z","shell.execute_reply.started":"2023-06-17T09:24:08.536460Z","shell.execute_reply":"2023-06-17T09:24:08.545686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-large-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:24:11.846844Z","iopub.execute_input":"2023-06-17T09:24:11.847732Z","iopub.status.idle":"2023-06-17T09:24:35.704038Z","shell.execute_reply.started":"2023-06-17T09:24:11.847689Z","shell.execute_reply":"2023-06-17T09:24:35.703152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model(bert_model)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:24:39.164042Z","iopub.execute_input":"2023-06-17T09:24:39.164408Z","iopub.status.idle":"2023-06-17T09:24:47.958112Z","shell.execute_reply.started":"2023-06-17T09:24:39.164378Z","shell.execute_reply":"2023-06-17T09:24:47.957036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:24:54.048620Z","iopub.execute_input":"2023-06-17T09:24:54.049013Z","iopub.status.idle":"2023-06-17T09:24:54.056244Z","shell.execute_reply.started":"2023-06-17T09:24:54.048982Z","shell.execute_reply":"2023-06-17T09:24:54.055371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Extract loss and accuracy values from the training history\n# loss = history['loss']\n# val_loss = history['val_loss']\n# accuracy = history['accuracy']\n# val_accuracy = history['val_accuracy']\n\n# # Plot the learning curves\n# plt.plot(loss, label='Training Loss')\n# plt.plot(val_loss, label='Validation Loss')\n# plt.plot(accuracy, label='Training Accuracy')\n# plt.plot(val_accuracy, label='Validation Accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Metrics')\n# plt.title('Learning Curves')\n# plt.legend()\n# plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\n\n# Save the model\nmodel.save('model.h5')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T09:25:01.071386Z","iopub.execute_input":"2023-06-17T09:25:01.072382Z","iopub.status.idle":"2023-06-17T09:25:03.526489Z","shell.execute_reply.started":"2023-06-17T09:25:01.072349Z","shell.execute_reply":"2023-06-17T09:25:03.525170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}